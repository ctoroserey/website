<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Claudio Toro-Serey on Claudio Toro-Serey</title>
    <link>/</link>
    <description>Recent content in Claudio Toro-Serey on Claudio Toro-Serey</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The one with my first hackathon and cortical parcellations.</title>
      <link>/post/hackathon2018/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/hackathon2018/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#so-you-signed-up-for-your-first-hackathon&#34;&gt;So you signed up for your first hackathon…&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ok-ok-but-what-did-you-actually-work-on&#34;&gt;Ok ok, but what did you actually work on?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#just-a-tad-more&#34;&gt;Just a tad more&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;so-you-signed-up-for-your-first-hackathon&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;So you signed up for your first hackathon…&lt;/h2&gt;
&lt;p&gt;About a year ago I was (luckily) invited to attend Neurohackademy at the University of Washington, Seattle. This was my first time even hearing about hackathons in neuroimaging, so I was definitely excited to check it out. The format of the 2-week event was perfect for me: one week of broad introductions to tools and perspectives that we were supposed to implement during the second week in collectively-idealized projects. While excited, the part of me perennially afflicted by the imposter syndrome never stopped asking &lt;strong&gt;“what could I possibly contribute to projects that other people can’t already do?”&lt;/strong&gt; Turns out not that much. But I also learned that hackathons are as much about learning as they are about contributing, and even small contributions are welcomed. So I’m writing this short post for those who, like me, might be nervous about the prospect of their first hackathon.&lt;/p&gt;
&lt;p&gt;Let’s get something out of the way: the core idea of a hackathon is appealing to people who are heavily invested in open, community driven scientific projects and tools. This, in my opinion, has two comforting consequences: (1) those around you will (most likely) be more into collaboration than competition; and (2) anyone who has worked in open science knows that projects seldom reach perfection (just go and look at the ‘Issues’ section of any open repo you admire). This coarsely translates into: &lt;strong&gt;you don’t have to prove yourself to anyone, and it’s ok if the projects you dig into are not at their ideal stage by the end of the hackathon.&lt;/strong&gt; Hopefully that takes some pressure off your shoulders.&lt;/p&gt;
&lt;p&gt;You might still be wondering whether your level of skill will match the requirements of the project(s) you affiliate with. After all, many of those around you &lt;em&gt;will&lt;/em&gt; have extensive knowledge of things like git, open-source programing languages, continuous integration, notebook implementation (e.g. Jupyter), package development, environment sharing tools, community guidelines, etc. Knowing that the collective knowledge of a room is miles ahead of mine has always been a reliable depressor for me (this makes living in Boston challenging at times). What if these experienced people expect basic expertise? Well, let me tell you about some of the things I (and other newbies) did for the first time during Neurohackademy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I issued my first pull request. I literally just added a file to &lt;a href=&#34;https://github.com/HBClab/NiBetaSeries&#34;&gt;NiBetaSeries&lt;/a&gt;, with James Kent patiently guiding me through the process. Before then I had just used GitHub to keep track of my own projects.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Python dominated the hackathon. I did not dominate Python. I was just getting used to R at that time, and I didn’t even know what a ‘class’ object was. Heck, I had never even loaded a custom package locally before someone at NH showed me how to.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Opened and edited a Jupyter notebook.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;My main group almost broke a repository, and Kirstie Whitaker spent a charitable couple of hours helping us fix it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Looked at C++ code (yes, looked. I didn’t even try to touch it).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Etc. A lot of etc.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many of these are what many would consider “basic knowledge”, yet plenty of such first encounters occurred. What’s more, it’s possible that some of these first encounters will be about things you &lt;em&gt;do&lt;/em&gt; know about, and will be able to help others with! As a personal example, someone in a different group had a question about R that I helped tackle to some degree. Super simple, I know, but hackathons are all about helping each other in both big and small ways. Every bit counts when a bunch of people stuck in a room are trying to push science forward. I guess what I’m trying to convery here is: &lt;strong&gt;as long as you come in with the right collaborating (and respectful) attitude, both you and others will get something out of this experience.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ok-ok-but-what-did-you-actually-work-on&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ok ok, but what did you actually work on?&lt;/h2&gt;
&lt;p&gt;Maybe describing my experience will make all of this more tangible.&lt;/p&gt;
&lt;p&gt;The main project I plugged myself into was already kind of in the oven before NH. Michael Notter had been working on a Python tool to parcellate a cortical surface (or a region within it) into &lt;em&gt;n&lt;/em&gt; equally-sized regions. He called it the &lt;a href=&#34;https://github.com/miykael/parcellation_fragmenter&#34;&gt;“parcellation fragmenter”&lt;/a&gt;. I liked this project because I had recently been learning about clustering algorithms for my own work, and there were talks about implementing one of my favorite ones into the clustering catalogue (spectral partitioning). Here’s an example of what this looks like for different surface shapes and densities:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/surface_fragments.png&#34; /&gt; Not only did I love the look of those fragmented brains, I also thought I could help with the addition of spectral partitioning. Well, that wasn’t the case. Kristian Eschenburg had it under control from the get-go, and he implemented much of the code and technical reframing of the package. Luckily for me, both him and Michael were happy to guide me through tractable issues and testing that I could do, which helped me learn Python better. Even though the work seemed inconsequential, it did benefit the project to some extent: I found a pair of hiccups in the calculation of distances between cortical vertices (super important for any clustering algorithm), among other tiny fixes.&lt;/p&gt;
&lt;p&gt;Even though I felt welcomed to the project and I loved learning, I also felt bad for not being able to contribute more substantially. At this point something clicked: &lt;strong&gt;I was trying to add content where someone else’s expertise worked better, but I wasn’t trying to help the project in new ways that I knew I was capable of&lt;/strong&gt;. That’s when I proposed something very simple to Michael and Kristian: what if we benchmark the algorithms you are implementing here? The idea was to add information on reliability and efficiency of the algorithms to help future users decide which clustering method to use. Fortunately they liked the idea. All that I did was produce the following three figures (and a short Jupyter notebook to display them along with their code)&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/cluster_comparison2.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Just so you’re not left hanging, this is what they mean: &lt;em&gt;A&lt;/em&gt; shows how long it takes each algorithm to produce brain parcellations as a function of the desired number of clusters. &lt;em&gt;B&lt;/em&gt; and &lt;em&gt;C&lt;/em&gt;, on the other hand, tell you how similar partitions are if you re-run the code 10 times for parcellations of 10 and 100 clusters (i.e. how reliable each method is). The similarities were computed using the adjusted Rand Index, which is explained in an older post. Perhaps unsurprisingly, this shows that k-means is fast, but also highly variable (which can be helpful if you want many slight variations for a permutation, for example), whereas Ward is both relatively fast and highly stable across iterations. Spectral partitioning is not shown because it’s extra slow, but also extra stable.&lt;/p&gt;
&lt;p&gt;So, what are the takeaways here? First, you can tell that a week of experience didn’t prevent me from making silly mistakes in Python, as I couldn’t quite get the order of the algorithms to match across the similarity matrices above. And second, that even something as straightforward as this can be a valued addition to a project, even if it doesn’t directly affect the way the package/tool/project works. As long as it’s relevant to the project and/or it helps you grow, then there is value in pursuing it (or at least proposing it).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;just-a-tad-more&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Just a tad more&lt;/h2&gt;
&lt;p&gt;So yes, hackathons are more than just rushing to complete a project, as I thought they would be before I attended Neurohackademy. Sure, many great projects will be pretty polished (&lt;a href=&#34;https://github.com/miykael/atlasreader&#34;&gt;here’s an example&lt;/a&gt;), but likely not all will. They’re a great place to learn and gain confidence to continue helping the open community at large. You’ll probably learn tools like &lt;em&gt;git&lt;/em&gt; that will be great to implement in your own work. Beyond that, you’ll get to meet a diverse group of people that might become collaborators in the future, or that you’ll look forward to seeing at future conferences (or at least you’ll get new twitter friends). Keep in mind that this post is based on my one experience (plus other people’s anecdotes), but if you’re concerned about fitting in at your first hackathon (like I was), hopefully this short post can help you get rid of those thoughts.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The one with my first hackathon, cortical parcellations, and algorithm evaluation.</title>
      <link>/post/hackathon2018/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/hackathon2018/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#so-you-signed-up-for-your-first-hackathon&#34;&gt;So you signed up for your first hackathon…&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ok-ok-but-what-did-you-actually-work-on&#34;&gt;Ok ok, but what did you actually work on?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#just-a-tad-more&#34;&gt;Just a tad more&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;so-you-signed-up-for-your-first-hackathon&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;So you signed up for your first hackathon…&lt;/h2&gt;
&lt;p&gt;About a year ago I was (luckily) invited to attend Neurohackademy at the University of Washington, Seattle. This was my first time even hearing about hackathons in neuroimaging, so I was definitely excited to check it out. The format of the 2-week event was perfect for me: one week of broad introductions to tools and perspectives that we were supposed to implement during the second week in collectively-idealized projects. While excited, the part of me perennially afflicted by the imposter syndrome never stopped asking &lt;strong&gt;“what could I possibly contribute to projects that other people can’t already do?”&lt;/strong&gt; Turns out not that much. But I also learned that hackathons are as much about learning as they are about contributing, and even small contributions are welcomed. So I’m writing this short post for those who, like me, might be nervous about the prospect of their first hackathon.&lt;/p&gt;
&lt;p&gt;Let’s get something out of the way: the core idea of a hackathon is appealing to people who are heavily invested in open, community driven scientific projects and tools. This, in my opinion, has two comforting consequences: (1) those around you will (most likely) be more into collaboration than competition; and (2) anyone who has worked in open science knows that projects seldom reach perfection (just go and look at the ‘Issues’ section of any open repo you admire). This coarsely translates into: &lt;strong&gt;you don’t have to prove yourself to anyone, and it’s ok if the projects you dig into are not at their ideal stage by the end of the hackathon.&lt;/strong&gt; Hopefully that takes some pressure off your shoulders.&lt;/p&gt;
&lt;p&gt;You might still be wondering whether your level of skill will match the requirements of the project(s) you affiliate with. After all, many of those around you &lt;em&gt;will&lt;/em&gt; have extensive knowledge of things like git, open-source programing languages, continuous integration, notebook implementation (e.g. Jupyter), package development, environment sharing tools, community guidelines, etc. Knowing that the collective knowledge of a room is miles ahead of mine has always been a reliable depressor for me (this makes living in Boston challenging at times). What if these experienced people expect basic expertise? Well, let me tell you about some of the things I (and other newbies) did for the first time during Neurohackademy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I issued my first pull request. I literally just added a file to &lt;a href=&#34;https://github.com/HBClab/NiBetaSeries&#34;&gt;NiBetaSeries&lt;/a&gt;, with James Kent patiently guiding me through the process. Before then I had just used GitHub to keep track of my own projects.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Python dominated the hackathon. I did not dominate Python. I was just getting used to R at that time, and I didn’t even know what a ‘class’ object was. Heck, I had never even loaded a custom package locally before someone at NH showed me how to.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Opened and edited a Jupyter notebook.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;My main group almost broke a repository, and Kirstie Whitaker spent a charitable couple of hours helping us fix it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Looked at C++ code (yes, looked. I didn’t even try to touch it).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Etc. A lot of etc.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many of these are what many would consider “basic knowledge”, yet plenty of such first encounters occurred. What’s more, it’s possible that some of these first encounters will be about things you &lt;em&gt;do&lt;/em&gt; know about, and will be able to help others with! As a personal example, someone in a different group had a question about R that I helped tackle to some degree. Super simple, I know, but hackathons are all about helping each other in both big and small ways. Every bit counts when a bunch of people stuck in a room are trying to push science forward. I guess what I’m trying to convery here is: &lt;strong&gt;as long as you come in with the right collaborating (and respectful) attitude, both you and others will get something out of this experience.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ok-ok-but-what-did-you-actually-work-on&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ok ok, but what did you actually work on?&lt;/h2&gt;
&lt;p&gt;Maybe describing my experience will make all of this more tangible.&lt;/p&gt;
&lt;p&gt;The main project I plugged myself into was already kind of in the oven before NH. Michael Notter had been working on a Python tool to parcellate a cortical surface (or a region within it) into &lt;em&gt;n&lt;/em&gt; equally-sized regions. He called it the &lt;a href=&#34;https://github.com/miykael/parcellation_fragmenter&#34;&gt;“parcellation fragmenter”&lt;/a&gt;. I liked this project because I had recently been learning about clustering algorithms for my own work, and there were talks about implementing one of my favorite ones into the clustering catalogue (spectral partitioning). Here’s an example of what this looks like for different surface shapes and densities:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/surface_fragments.png&#34; /&gt; Not only did I love the look of those fragmented brains, I also thought I could help with the addition of spectral partitioning. Well, that wasn’t the case. Kristian Eschenburg had it under control from the get-go, and he implemented much of the code and technical reframing of the package. Luckily for me, both him and Michael were happy to guide me through tractable issues and testing that I could do, which helped me learn Python better. Even though the work seemed inconsequential, it did benefit the project to some extent: I found a pair of hiccups in the calculation of distances between cortical vertices (super important for any clustering algorithm), among other tiny fixes.&lt;/p&gt;
&lt;p&gt;Even though I felt welcomed to the project and I loved learning, I also felt bad for not being able to contribute more substantially. At this point something clicked: &lt;strong&gt;I was trying to add content where someone else’s expertise worked better, but I wasn’t trying to help the project in new ways that I knew I was capable of&lt;/strong&gt;. That’s when I proposed something very simple to Michael and Kristian: what if we benchmark the algorithms you are implementing here? The idea was to add information on reliability and efficiency of the algorithms to help future users decide which clustering method to use. Fortunately they liked the idea. All that I did was produce the following three figures (and a short Jupyter notebook to display them along with their code)&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/cluster_comparison2.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Just so you’re not left hanging, this is what they mean: &lt;em&gt;A&lt;/em&gt; shows how long it takes each algorithm to produce brain parcellations as a function of the desired number of clusters. &lt;em&gt;B&lt;/em&gt; and &lt;em&gt;C&lt;/em&gt;, on the other hand, tell you how similar partitions are if you re-run the code 10 times for parcellations of 10 and 100 clusters (i.e. how reliable each method is). The similarities were computed using the adjusted Rand Index, which is explained in an older post. Perhaps unsurprisingly, this shows that k-means is fast, but also highly variable (which can be helpful if you want many slight variations for a permutation, for example), whereas Ward is both relatively fast and highly stable across iterations. Spectral partitioning is not shown because it’s extra slow, but also extra stable.&lt;/p&gt;
&lt;p&gt;So, what are the takeaways here? First, you can tell that a week of experience didn’t prevent me from making silly mistakes in Python, as I couldn’t quite get the order of the algorithms to match across the similarity matrices above. And second, that even something as straightforward as this can be a valued addition to a project, even if it doesn’t directly affect the way the package/tool/project works. As long as it’s relevant to the project and/or it helps you grow, then there is value in pursuing it (or at least proposing it).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;just-a-tad-more&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Just a tad more&lt;/h2&gt;
&lt;p&gt;So yes, hackathons are more than just rushing to complete a project, as I thought they would be before I attended Neurohackademy. Sure, many great projects will be pretty polished (&lt;a href=&#34;https://github.com/miykael/atlasreader&#34;&gt;here’s an example&lt;/a&gt;), but likely not all will. They’re a great place to learn and gain confidence to continue helping the open community at large. You’ll probably learn tools like &lt;em&gt;git&lt;/em&gt; that will be great to implement in your own work. Beyond that, you’ll get to meet a diverse group of people that might become collaborators in the future, or that you’ll look forward to seeing at future conferences (or at least you’ll get new twitter friends). Keep in mind that this post is based on my one experience (plus other people’s anecdotes), but if you’re concerned about fitting in at your first hackathon (like I was), hopefully this short post can help you get rid of those thoughts.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Modeling Diagnostic Expectations</title>
      <link>/post/diagexp/</link>
      <pubDate>Mon, 04 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/diagexp/</guid>
      <description>&lt;p&gt;The biggest appeal of behavioral economics is its promise of practically regularizing the rational expectations imposed by traditional economics. Since the 70s psychology has catalogued increasing amounts of biases and circumstancial heterogeneities that go against the expectation of rationality in decision making. However, in my short years studying it, it seems like these discoveries are seldom applied to complex real-life economic environments (Nudge being a good example of this). That’s what attracted me to ‘A Crisis of Beliefs’, a book by Nicola Gennaioli and Andrei Shleifer that was published last year.&lt;/p&gt;
&lt;p&gt;In this book, G&amp;amp;S apply the representativeness heuristic to model the optimistic expectations of bankers and real estate investors in the years prior to the 2008 recession, and suggest that these distorted beliefs are partially to blame for the ensuing financial crisis. In contrast with Rational Expectations from pure economics, the authors call their ‘human friendly’ model Diagnostic Expectations. I’m not too familiar with traditional economics, but I found the book very readable, instructive, and enjoyable. Actually, I liked their model and explanations so much that I decided to create a &lt;a href=&#34;https://ctoroserey.shinyapps.io/DiagnosticExp/&#34;&gt;Shiny App that lets you interact with the economic factors that they propose&lt;/a&gt;. In general, I’m a big fan of dynamically visualizing models, as static figures can sometimes fail to convey all potentially interesting parameterizations. In this case, I think the app allows you to experience all the relevant scenarios from the book.&lt;/p&gt;
&lt;p&gt;A failry detailed (but still simplistic) explanation of the model components can be found in the Shiny app, so I won’t repeat it here. Instead, I’ll just leave a snippet of how the app works.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/diag_exp.gif&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The authors are clear about a number of shortcomings in their environmental setup. For example, there is no account for house price fluctuation, and no account for what a participant’s theta should be given the context (among others). These are all interesting prospective avenues to examine.&lt;/p&gt;
&lt;p&gt;Eventually I might expand this blog post to mention some aspects of the model that weren’t clear or well addressed to me. For example, the discount of future returns is linear, which goes against the hyperbolic devaluation we’re used to in behavioral economics. It is also unclear why actors have heterogenous risk attitudes. Given the same environment all participants should be similarly risk-averse or prone. This dichotomy could be a function of a participant’s perspective. For example, under prospect theory, investors might see debt purchasing in the realm of gains, while bankers understand it as potential losses (thus explaining their risk-insensitivity), or perhaps a case of weighting gains and losses in a way that produces these effects. But for now, I think the app is enough.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vector Operations and Linear Models</title>
      <link>/post/lmmath/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/lmmath/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vector-and-matrix-mutiplication&#34;&gt;Vector and matrix mutiplication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As a psychology student I (unfortunately) wasn’t told to take any math courses during my undergrad, and by the time I was starting my Ph.D. I had forgotten most of the math I learned in high school. Well, unlike what I was told in undergrad, psychological research has become quite mathy, and its interaction with cognitive and computational neuroscience means that you are very likely to run into the linear algebra and calculus that your teenager self swore would never become useful. I have thus had to reacquaint myself with math in order to understand the behavioral, neural, and network modeling I read about and use in my own work.&lt;/p&gt;
&lt;p&gt;But my math problem really stemmed from something quite simple: wanting to understand the mathematical components behind multiple regression. Sure, running and interpreting a linear model doesn’t require math proficiency (many psych students are taught how to click their way through ANOVAs, after all), but knowing what goes on under the algebraic hood has two big benefits:&lt;/p&gt;
&lt;p&gt;1.- Understanding the model’s limitations&lt;/p&gt;
&lt;p&gt;2.- Preparing you for more complex statistics and machine learning algorithms that rely on similar intuitions&lt;/p&gt;
&lt;p&gt;Learning about this brings about the other half of this post’s title: vector/matrix operations. If you Google the equation that solves for multiple regression, you’re hit in the face with this element of linear algebra that psych students like me probably forgot about (well, you can bypass this by doing gradient descent, but that’s a different post). As it turns out, understanding matrix notation is necessary once you get into more complex and niche tools that you might use in your research, and vectorizing problems might make you a better coder too (especially to avoid slow for-loops in R).&lt;/p&gt;
&lt;p&gt;All of this motivated me to write the post I wish I had found during my first year of grad school. Here I’ll try to build the intuition behind a linear model from the understanding of matrix operations. This post is aimed for math-deficient psych students like me who want a basic introduction to both of these things without having to jump across tutorials. I hope to eventually turn this into a series of approachable tutorials for other statistical tools.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vector-and-matrix-mutiplication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vector and matrix mutiplication&lt;/h2&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Secret Santa Generator Shiny App</title>
      <link>/post/ssgnrtr/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ssgnrtr/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#why-are-you-doing-this-there-are-enough-secret-santa-services&#34;&gt;Why are you doing this? There are enough secret santa services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#so-how-does-it-work&#34;&gt;So, how does it work?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#shiny-implementation&#34;&gt;Shiny implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#closing&#34;&gt;Closing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;why-are-you-doing-this-there-are-enough-secret-santa-services&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why are you doing this? There are enough secret santa services&lt;/h2&gt;
&lt;p&gt;Last Christmas my wife was left giftless on my side of the family due to a faulty secret santa generator. In trying to earn brownie points the arrogant nerd in me thought he could do better, so I decided to program one in R (I’d rather know exactly what’s going on in the background anyways). I also found it hard this year to find a service that prevented specific pairs of people from gifting each other.&lt;/p&gt;
&lt;p&gt;But beyond my petty personal reasons, a friend suggested that this would be a good opportunity to learn how to build Shiny apps. So I’ll use this post to explain the basics of the code, and how you can turn it into a Shiny app (disclaimer: this being my first Shiny app means that the setup is crude, so I definitely recommend checking out the great official examples that Shiny provides).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update 11/30/18: of course I found &lt;a href=&#34;https://blog.revolutionanalytics.com/2017/11/how-to-generate-a-secret-santa-list-with-r.html&#34;&gt;this blog post&lt;/a&gt; the day after I posted mine, which shows that the script and idea had been mostly done before by a combination of coders. Even some of the language that I have here ended up being too similar to what David writes, unfortunately. Even more, &lt;a href=&#34;https://www.tjmahr.com/secret-santa-graph-traversal/&#34;&gt;Tristan Mahr already solved the problem of pair constraints&lt;/a&gt; through a graph implementation that, while too busy for my taste and to implement in Shiny, made for a pretty cool post. Still, I’ll keep this up since there are some novel elements to the way I went about it (that I know of, at least), and I couldn’t find another Shiny app that did the trick.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;so-how-does-it-work&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;So, how does it work?&lt;/h2&gt;
&lt;p&gt;The setup is simple. Let’s look at the function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xmaspairs &amp;lt;- function(Members = 1, Spouses = 1, Secret = T) {
  
  # Make sure that members were provided..
  if (length(Members) == 1) {stop(&amp;quot;No members indicated...&amp;quot;)}
  
  # If spouse pairing should be avoided..
  if (length(Spouses) == length(Members)) {
    
    # Make sure that a single group isn&amp;#39;t more than 50% of the members
    if(max(table(input$Spouses)) &amp;gt; length(input$Members)/2) {stop(&amp;quot;A group can&amp;#39;t be more than 50% of the total members...&amp;quot;)}
    
    # Iterate over possible pairings until no SOs are paired
    Affiliated &amp;lt;- T
    while (Affiliated) {
      m &amp;lt;- sample(Members)
      df &amp;lt;- data.frame(Member = m,
                       Gift_to = c(tail(m, n = 1), m[seq(length(m)-1)])) # sure there&amp;#39;s a better way to do this..
      Spouse1 &amp;lt;- Spouses[match(df$Member, Members)]
      Spouse2 &amp;lt;- Spouses[match(df$Gift_to, Members)]
      Affiliated &amp;lt;- T %in% (Spouse1 == Spouse2)
    }
  # Otherwise just produce whatever pairing comes out from a single sampling  
  } else { 
    m &amp;lt;- sample(Members)
    df &amp;lt;- data.frame(Member = m,
                     Pair = c(tail(m, n = 1), m[seq(length(m)-1)]))
  }
  
  # If the person running it should be blinded to the pairs, create individual txt files
  if (Secret) {
    for (i in seq(nrow(df))) {
      write.table(paste(&amp;quot;Your secret santa is: &amp;quot;, df[i, 2], &amp;quot;!&amp;quot;, sep = &amp;quot;&amp;quot;), 
                  file = paste(df[i,1],&amp;quot;.txt&amp;quot;, sep = &amp;quot;&amp;quot;),
                  row.names = F, 
                  col.names = F)
    }
  # or for groups who don&amp;#39;t care about social subtleties
  } else {
    return(df)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Basically, the function takes in a mandatory character vector of members, an optional grouping vector (either numeric of string), and the choice of whether to hide the final pairings from everyone. The way it works is simple: shuffle the members, then create a new vector with everyone shifted by 1 position (this is the equivalent of randomly sitting people in a circle and telling them to gift the person to their right).&lt;/p&gt;
&lt;p&gt;Now, if you wanted to prevent 2 members from gifting each other, all you do is feed a grouping vector into Spouses. So, let’s say you have Mom, Dad, Cindy, you, and Dormamu the Destroyer on your members list, and of course you don’t want your parents gifting each other. Then all you have to do is create a vector like &lt;code&gt;c(Parent, Parent, 1, 2, Immortal)&lt;/code&gt; to feed to the function. This will force the function to iterate through combinations of gifters/receivers until the pairs don’t share the same grouping characteristic. Note that since I use an equivalence, pretty much anything can be used as a grouping variable. If you don’t care about this, just write NA or anything that doesn’t match the length of the members list.&lt;/p&gt;
&lt;p&gt;Finally, if you select &lt;code&gt;Secret = T&lt;/code&gt; the function won’t generate a dataframe. Instead, it will download a text file for each member on your current directory, indicating to whom they should gift. I know it would be easier to email people rather than having to send them their text files manually, but I was a bit lazy and didn’t want to deal with SMTP setups.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny-implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shiny implementation&lt;/h2&gt;
&lt;p&gt;It might be helfpul to get acquainted with how everything looks before digging into the code. You can go ahead and play with the app if you’d like, or check it out &lt;a href=&#34;https://ctoroserey.shinyapps.io/SecretSantaGeneratR/&#34;&gt;online&lt;/a&gt;.&lt;/p&gt;
&lt;iframe width=&#34;100%&#34; height=&#34;900&#34; src=&#34;https://ctoroserey.shinyapps.io/SecretSantaGeneratR/&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;Alright then, how is it made?&lt;/p&gt;
&lt;p&gt;The core of a Shiny app is way simpler than I expected. You have three components: a &lt;code&gt;ui&lt;/code&gt; to dictate the arrangement of elements, a &lt;code&gt;server&lt;/code&gt; to produce its contents, and a simple &lt;code&gt;shinyApp&lt;/code&gt; function that brings these two together.&lt;/p&gt;
&lt;p&gt;This is what the &lt;code&gt;ui&lt;/code&gt; portion looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ui &amp;lt;- fluidPage(
#   
#   titlePanel(&amp;quot;Secret Santa GeneratR&amp;quot;),
# 
#   p(&amp;quot;Welcome to yet another secret santa generator! The advantage of this one is that it gives you the option to keep the pairings secret or not, as well as avoiding pairs of people who should not gift each other! Perfect if you have inter-dimensional friends that can&amp;#39;t physically interact with each other.&amp;quot;),
#   
#   #Sidebar layout with input and output definitions ----
#   sidebarLayout(
# 
#     # Sidebar panel for inputs ----
#     sidebarPanel(
# 
#       # Input: Text for providing a caption ----
#       textInput(inputId = &amp;quot;Members&amp;quot;,
#                 label = &amp;quot;Group member names:&amp;quot;,
#                 value = &amp;quot;A, B, C, D, E, F&amp;quot;),
#       
#       # Explanation of spouse matching
#       p(&amp;quot;If you want to prevent specific pairs of people from gifting each other, write down some characteristic that pairs them below (in the order they&amp;#39;re written above). In the example below, A/B and C/D are part of &amp;#39;Couple&amp;#39; and &amp;#39;Couple2&amp;#39;, respectively, and won&amp;#39;t gift within couples; but E and F have their own group and can give/receive with anyone (note that the number of entries must match the number of members). Write &amp;#39;NA&amp;#39; if you don&amp;#39;t care about this.&amp;quot;),
#       
#       # Set pairs to avoid
#       textInput(inputId = &amp;quot;Spouses&amp;quot;,
#                 label = &amp;quot;Avoidance list:&amp;quot;,
#                 value = &amp;quot;Couple, Couple, Couple2, Couple2, S1, S2&amp;quot;),
#       
#      # Apply changes
#      submitButton(&amp;quot;Update List&amp;quot;),
#      
#      # Empty space
#      p(),
#
#       # Note on making it secret
#       p(&amp;quot;If you want to keep it secret, a file for each member will be created telling them who they should gift based on a new, unseen pairing. Just send each person their file!&amp;quot;),
#       
#       # and the respective download button for the zip file
#       downloadButton(&amp;quot;download&amp;quot;, &amp;quot;Make Secret&amp;quot;)
# 
#     ),
# 
#     # Main panel for displaying outputs ----
#     mainPanel(
# 
#       p(&amp;quot;Here are the current santa pairs. Note that if you click on &amp;#39;Make Secret&amp;#39; a completely new scheme will be produced that you won&amp;#39;t see here.&amp;quot;),
#       
#       # Output: HTML table with requested number of observations ----
#       tableOutput(&amp;quot;view&amp;quot;)
# 
#     )
#   )
# )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All we are doing here is defining what goes up top (&lt;code&gt;titlePanel&lt;/code&gt; and the first paragraph &lt;code&gt;p&lt;/code&gt;), which user inputs to place in the sidebar (&lt;code&gt;sidebarPanel&lt;/code&gt; within &lt;code&gt;sidebarLayout&lt;/code&gt;), and what goes on the main area of the site (&lt;code&gt;mainPanel&lt;/code&gt; within &lt;code&gt;sidebarLayout&lt;/code&gt;). Things to note here: functions like &lt;code&gt;textInput&lt;/code&gt; ask the user for input, and you give them IDs so you can call the values in the &lt;code&gt;server&lt;/code&gt; portion (see next). Here I place default values just to give the user an example. On the other hand, &lt;code&gt;tableOutput&lt;/code&gt; and &lt;code&gt;downloadButton&lt;/code&gt; seem to be getting input from somewhere (here “view” and “download”). These are output variables generated by the server! So all that’s happening is that the &lt;code&gt;ui&lt;/code&gt; passes on inputs that are then processed and returned by the &lt;code&gt;server&lt;/code&gt; to be placed on the GUI. That’s it.&lt;/p&gt;
&lt;p&gt;One last thing about &lt;code&gt;ui&lt;/code&gt; and Shiny in general: these widgets are reactive, meaning that they will update the output whenever any changes are made. This can be problematic, so we add a &lt;code&gt;submitButton&lt;/code&gt; called ‘Update List’ to apply the changes the user makes.&lt;/p&gt;
&lt;p&gt;Next, what does the &lt;code&gt;server&lt;/code&gt; look like?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# server &amp;lt;- function(input, output){
#   
#   
#   # Function that does the pairings
#   xmaspairs &amp;lt;- function(Members = 1, Spouses = 1, Secret = T) {
#     
#     # This crude function will pair group members for secret santa
#     # If your family would like to avoid pairing significant others,
#     # define Spouses to be a vector of groupings (i.e. numeric).
#     # The script will then iterate over pairings until no SOs are paired.
#     
#     # Perform the actual pairing
#     # If spouse pairing should be avoided..
#     if (length(Spouses) == length(Members)) {
#       # Once this turns false, we are in business
#       Affiliated &amp;lt;- T
#       # Iterate over possible pairings until no SOs are paired
#       while (Affiliated) {
#         m &amp;lt;- sample(Members)
#         df &amp;lt;- data.frame(Member = m,
#                          Gift_to = c(tail(m, n = 1), m[seq(length(m)-1)])) # sure there&amp;#39;s a better way to do this..
#         Spouse1 &amp;lt;- Spouses[match(df$Member, Members)]
#         Spouse2 &amp;lt;- Spouses[match(df$Gift_to, Members)]
#         Affiliated &amp;lt;- T %in% (Spouse1 == Spouse2)
#       }
#     # Otherwise just produce whatever pairing comes out from a single sampling  
#     } else { 
#       m &amp;lt;- sample(Members)
#       df &amp;lt;- data.frame(Member = m,
#                        Gift_to = c(tail(m, n = 1), m[seq(length(m)-1)]))
#     }
#     
#     # spit out the pairs
#     return(df)
#     
#   }
#   
#   
#   # Download a zip file with each pair 
#   output$download &amp;lt;- downloadHandler(
#     
#     # Name of the download
#     filename = function() {&amp;quot;SecretSantaPairs.zip&amp;quot;},
#     
#     # Prep the zip file 
#     content = function(file) { 
#       # Parse the strings
#       m &amp;lt;- unlist(strsplit(gsub(&amp;quot; &amp;quot;, &amp;quot;&amp;quot;, input$Members, fixed=T), &amp;quot;,&amp;quot;))
#       s &amp;lt;- unlist(strsplit(gsub(&amp;quot; &amp;quot;, &amp;quot;&amp;quot;, input$Spouses, fixed=T), &amp;quot;,&amp;quot;))
#       # Make sure the groupings don&amp;#39;t break the code by having a group be over 50% of members
#       validate(need(try(max(table(s)) &amp;lt; length(m)/2), &amp;quot;A group can&amp;#39;t have more than 50% of the members&amp;quot;))
#       #Pairing
#       df &amp;lt;- xmaspairs(Members = m, 
#                       Spouses = s)
#       # Write files per person indicating to whom they have to gift
#       owd &amp;lt;- setwd(tempdir()) # temporary dir to store the files
#       on.exit(setwd(owd))
#       files &amp;lt;- list()      
#       lapply(seq(nrow(df)), function(i) {
#         write.table(paste(&amp;quot;Your secret santa is: &amp;quot;, df[i, 2], &amp;quot;!&amp;quot;, sep = &amp;quot;&amp;quot;), 
#                     file = paste(df[i,1],&amp;quot;.txt&amp;quot;, sep = &amp;quot;&amp;quot;),
#                     row.names = F, 
#                     col.names = F)})
#       zip(file, paste(unlist(strsplit(gsub(&amp;quot; &amp;quot;, &amp;quot;&amp;quot;, input$Members, fixed=T), &amp;quot;,&amp;quot;)), &amp;quot;.txt&amp;quot;, sep = &amp;quot;&amp;quot;)) # and zip
#     }
#     
#   )
#   
#   
#     # render the resulting table 
#   output$view &amp;lt;- renderTable({
#     
#       # Parse the strings
#       m &amp;lt;- unlist(strsplit(gsub(&amp;quot; &amp;quot;, &amp;quot;&amp;quot;, input$Members, fixed=T), &amp;quot;,&amp;quot;))
#       s &amp;lt;- unlist(strsplit(gsub(&amp;quot; &amp;quot;, &amp;quot;&amp;quot;, input$Spouses, fixed=T), &amp;quot;,&amp;quot;))
#       # Make sure that enough members are entered
#       validate(need(length(m) &amp;gt; 1, &amp;quot;Not enough members!&amp;quot;))
#       # Make sure the groupings don&amp;#39;t break the code by having a group be over 50% of members
#       validate(need(try(max(table(s)) &amp;lt;= length(m)/2), &amp;quot;A group can&amp;#39;t have more than 50% of the members&amp;quot;))
#       # Produce pairings to display
#       xmaspairs(Members = m, 
#                 Spouses = s,
#                 Secret = input$Secret)
#     
#   })
# }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You probably already saw that the &lt;code&gt;server&lt;/code&gt; function works with input and output, as we just discussed. The next thing you’ll see is a shortened version of the &lt;code&gt;xmaspairs&lt;/code&gt; function, but without the ability to download files. Why is that? Well, Shiny comes with a handy widget to download files, and this saves us the pain of asking the user to write down a local path that the app might not have access to. The way this works is that you’ll store what to return to the &lt;code&gt;ui&lt;/code&gt; in an &lt;code&gt;outputs&lt;/code&gt; list. First we define &lt;code&gt;outputs$download&lt;/code&gt; (this is the “download” that gets passed onto &lt;code&gt;downloadButton&lt;/code&gt; in the &lt;code&gt;ui&lt;/code&gt; above). This widget requires you to define the &lt;code&gt;filename&lt;/code&gt; (“SecretSantaPairs.zip”) and the &lt;code&gt;content&lt;/code&gt;. Within &lt;code&gt;content&lt;/code&gt; you can write whatever code will generate the information that you want the user to download. All I did here was to run the file-writing part of my original function separately from creating the pairs, storing the files on a temporary directory, and compressing them into a single zip file that will be downloaded (this is because &lt;code&gt;downloadButton&lt;/code&gt; can only handle one file at a time). Note that &lt;code&gt;xmaspairs&lt;/code&gt; takes in the &lt;code&gt;ui&lt;/code&gt; values contained within the &lt;code&gt;inputs&lt;/code&gt; list (with the IDs that we defined above as the variable names). This is a cleaner way to cue the user to produce a secret listing.&lt;/p&gt;
&lt;p&gt;At the end of server you’ll find &lt;code&gt;renderTable&lt;/code&gt; being stored within &lt;code&gt;output$view&lt;/code&gt; (yes, the “view” that’s used in the &lt;code&gt;ui&lt;/code&gt;). This will show a table with the pairings according to the conditions specified. This will always appear (even if we click the download button), but note that since pressing the button generates an independent list, the user is still technically blinded from knowing who got who. Also worth noting is that I used &lt;code&gt;validate(need())&lt;/code&gt; to send out an error message to the user if either not enough members were entered, or if too many members were grouped together. If you use R’s classic &lt;code&gt;stop&lt;/code&gt;, Shiny will throw an ugly crashing error once published.&lt;/p&gt;
&lt;p&gt;Ok, now that we set up our interacting &lt;code&gt;server&lt;/code&gt; and &lt;code&gt;ui&lt;/code&gt; we can put them together. All you need to do is run&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(shiny)
# shinyApp(ui = ui, server = server) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open a new window with your app! (normally you put all these elements within the same file: app.R) This will also give you the option to publish you app to &lt;a href=&#34;https://www.shinyapps.io&#34;&gt;shinyapps.io&lt;/a&gt; by clicking ‘publish’ on your pop-up window. Of course this requires registering for an account there, but otherwise Shiny makes it seemless to upload your app so you can share it with everyone!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;closing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Closing&lt;/h2&gt;
&lt;p&gt;There are enough great tutorials on how to build Shiny apps, but I hope this will be useful for someone. If anything, there is one thing I would like whoever reads this to know: &lt;em&gt;You don’t need an extravagant project to try something new.&lt;/em&gt; If you would like to play with the code, it’s available on &lt;a href=&#34;https://github.com/ctoroserey/SS_GeneratR&#34;&gt;my GitHub site&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cluster comparisons with ARI</title>
      <link>/post/ari/</link>
      <pubDate>Wed, 26 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ari/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a-bit-of-background&#34;&gt;A bit of background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-rand-index&#34;&gt;The Rand Index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-adjusted-rand-index&#34;&gt;The Adjusted Rand Index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#closing&#34;&gt;Closing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;a-bit-of-background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A bit of background&lt;/h2&gt;
&lt;p&gt;A common need for researchers that rely on clustering algorithms, such as the organization of networks into cohesive node communities, is to evaluate the similarity of the partitions produced. In my case this problem takes the form of comparing the distribution of brain networks across individuals. While many tools have been developed to tackle the challenge (see Fortunato &amp;amp; Hric, 2016 for an initial survey), here I’ll give a superficial view on the adjusted rand index (ARI), hoping to better understand its behavior and ideal case usage. Since I’m more familiar with the usage of this measure in networks, I’ll base my terminology on this framework.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-rand-index&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Rand Index&lt;/h2&gt;
&lt;p&gt;The original Rand Index (RI) was introduced by William Rand in 1975, and it aimed to determine whether two clustering algorithms grouped every pair of nodes in a similar fashion. For example, it could be that nodes 3 and 25 from your network were grouped together in partitions from algorithms A and B, but separately by method C (you can probably already think of uses for this index in regards to examining the effects of experimental network perturbations in their organization). This agreement is computed by the following simple formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{a + b}{a + b + c + d}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is the number of pairs of nodes that were grouped together in both partitions, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is the number that were grouped separately, and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; denote the number grouped together (separately) in one partition, but separately (together) in the other. In short, the Rand Index just gives the proportion of nodes equally paired in both partitions.&lt;/p&gt;
&lt;p&gt;At this point you might have already thought: what is the advantage of doing this instead of directly checking if the node labels are identical across partitions? (i.e. &lt;code&gt;partitionA == partitionB&lt;/code&gt;) For some of us the answer is not quite evident, which is what motivated me to write this post. To begin exploring this, let’s look at a toy example of two hypothetical binary clustering algorithms (A and B) applied to a small network (I will mainly focus on this type of classification for simplicity):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Toy clusterings
table &amp;lt;- data.frame(A = c(1,1,1,0,0),
                    B = c(0,0,0,1,1))

# Output table
table %&amp;gt;% 
  knitr::kable(&amp;quot;html&amp;quot;) %&amp;gt;%
  kable_styling(full_width = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
B
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute agreement (arandi comes from the package &amp;#39;mcclust&amp;#39;)
EQ &amp;lt;- mean(table$A == table$B)
RI &amp;lt;- arandi(table$A, table$B, adjust = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, a simple equivalence (EQ) would of course yield a value of 0, as none of the values match. On the other hand, RI yields a value of 1. This is because the relationship among all nodes remains unchanged across partitions. This portrays the first advantage of metrics like RI: &lt;em&gt;they evaluate the underlying relationship among your clustered nodes while being agnostic to the labeling system.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now let’s tweak the clusters a bit, so that there is one rogue node that switches clusters.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
B
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In this case, checking the equivalence of the vectors yields 0.8, while the Rand Index gives 0.6. This is because RI doubles the dimensionality by considering the underlying interconnectedness of the nodes, such that the total number of pairwise combinations becomes &lt;code&gt;factorial(5) / (factorial(2) * factorial(5-2))&lt;/code&gt;, or 10. By changing the allegiance of a single node, we basically change the interaction of that node with the other four members, giving us an agreement level between partitions of &lt;code&gt;6/10&lt;/code&gt; (i.e., the RI value we got). To clarify this idea, let’s now suppose that cluster labels can be 0, 1, or 2, with the rogue node now being part of 2:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
B
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In this case, both the vector equivalence and RI give the same value (0.8). This is because the rogue node was originally tied to two others in cluster 0, whereas its unrelatedness to cluster 1 remains unchanged (unlike before, where it became part of it). This now gives a ratio of similarities of &lt;code&gt;8/10&lt;/code&gt;. Hopefully this provides a better sense of the advantages of RI over simple equivalence, especially once you increase both the number of nodes and clusters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-adjusted-rand-index&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Adjusted Rand Index&lt;/h2&gt;
&lt;p&gt;So far so good, but as researchers we always want to control for the possibility that our results are defined by chance! That’s where the adjustment to RI introduced by Hubert &amp;amp; Arabie (1985) comes in. The Adjusted Rand Index controls for the expected distribution of labels given by chance, and compares the original RI to this random model–while still guaranteeing that perfect clustering matches get a value of 1. &lt;a href=&#34;https://davetang.org/muse/2017/09/21/adjusted-rand-index/&#34;&gt;Others have already done a great job in explaining the mathematical nuances of the ARI&lt;/a&gt;, so I will instead focus on understanding how the three metrics (i.e. EQ, RI, and ARI) behave under similar circumstances.&lt;/p&gt;
&lt;p&gt;Let’s say we have an original vector of 100 nodes classified as part of either cluster 1 or 0. I will progressively switch the allegiance of each node, and on each case I will estimate the agreement between the new vector and the original one. In other words, I will check the values given by these metrics as the modified vector slowly becomes more disimilar from the original one. The plot summarizes the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Produce a random vector with 100 binary classifications
n &amp;lt;- 100
v1 &amp;lt;- rbinom(n = n, size = 1, prob = 0.5)

# Data frame to store evaluations
evals &amp;lt;- data.frame(Diffs = seq(n),
                    ARI = rep(0,n),
                    RI = rep(0,n),
                    EQ = rep(0,n))

# Check the degree of agreement per metric for each incremental switch in affiliation
for (i in seq(n)) {
  
  # Switch affiliation up to the ith node
  ifelse(i &amp;lt; n, v2temp &amp;lt;- c((1 - v1[seq(i)]), v1[(i+1):n]), v2temp &amp;lt;- v1)
  
  # Evaluate
  evals$ARI[i] &amp;lt;- arandi(v1,v2temp)
  evals$RI[i] &amp;lt;- arandi(v1,v2temp, adjust = F)
  evals$EQ[i] &amp;lt;- mean(v1 == v2temp)
  
}

# Make the data.frame long instead of wide for ggplot
evals &amp;lt;- melt(evals[seq(n-1),], id.vars = &amp;quot;Diffs&amp;quot;)

# And plot
ggplot(data = evals, aes(Diffs, value, group = variable, color = variable)) + 
  geom_line() +
  scale_color_discrete(name = &amp;quot;Method&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-26-ARI_files/figure-html/Compute-1.svg&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, progressively switching the affiliations of the original vector has the expected negative linear trend on EQ, as the vectors become increasingly dissimilar. On the other hand, RI heavily penalises initial clustering disagreements, but notice that once the differences pass 50% the vectors are deemed increasingly similar. This parabolic function is a result of node pairs once again being grouped in the same/different clusters once direct differences pass chance levels (think back to the first example, and how inverting the values still maintains the underlying relationships). Finally, we can see that ARI drops to 0 when the vector differences reach this chance level. In accord with its definition, positive ARI values thus denote agreements above those expected by chance&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;closing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Closing&lt;/h2&gt;
&lt;p&gt;As I said in the introduction, I use the ARI to check how similar the distribution of functional brain networks is across individuals. However, you could imagine using this metric as a way to evaluate the accuracy of neural nets as they are being trained (provided that you have the ground truth of what’s being clustered). It is also worth mentioning that the ARI is not the ultimate way to evaluate clusterings. As Fortunato &amp;amp; Hric (2016) discuss, metrics based on mutual information tend to be more robust (i.e. variation of information), and are also included in statistical packages (like mcclust for R, which I use here).&lt;/p&gt;
&lt;p&gt;Regardless, I hope this can eventually be as useful for someone to read as it was for me to write it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;If you are interested in getting the statistical significance of this deviation from chance, see Fortunato &amp;amp; Hric (2016) who discuss the computation of Z-scores based on permutations. I might expand this post in the future to show how this works.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>/project/deep-learning/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>/project/deep-learning/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;

&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;

&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;

&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;

&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>/project/example-external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>/project/example-external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A short introduction</title>
      <link>/post/getting-started/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>/post/getting-started/</guid>
      <description>&lt;p&gt;In a way, this is just a post to baptise the website. What I plan to do here from now on is to share snippets of what I&amp;rsquo;ve learned, read, or listened to lately (with a small dose of personal experiences), in hopes to both consolidate my knowledge and potentially help others. In a way, the picture of Boston in the header just represents that the overarching theme will be anything that has happened since I moved here for grad school (so yes, learning, reading, and listening to music mostly).&lt;/p&gt;

&lt;p&gt;Nothing too serious or big, just fun.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
